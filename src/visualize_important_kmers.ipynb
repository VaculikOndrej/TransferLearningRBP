{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QEZ_IFaBE4ha"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import tokenizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.att_layer import Attention \n",
    "from utils.data_loader import load_and_prepare\n",
    "\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import logomaker\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_based_on_pred_score(pred_list, asc_or_desc):\n",
    "    reverse = asc_or_desc\n",
    "    return(sorted(pred_list, key = lambda x: x[2], reverse=reverse))\n",
    "\n",
    "\n",
    "def replace_ts_and_same_len(list_of_seqs, min_length, same_len=True):\n",
    "    for i, el in enumerate(list_of_seqs):\n",
    "        if same_len:\n",
    "            act_len = len(el)\n",
    "            mid = act_len // 2\n",
    "            start = mid - min_length // 2\n",
    "            stop = mid + min_length // 2\n",
    "            list_of_seqs[i] = el[start:stop].lower().replace('t','u').upper()\n",
    "        else:\n",
    "            list_of_seqs[i] = el.lower().replace('t','u').upper()   \n",
    "    return list_of_seqs\n",
    "\n",
    "def get_important_subseqs_and_corresp_seqs(tokenizer_path, seq, cons, sec, model, subseq_len=20, number_of_seqs=50):\n",
    "    tokenizer = tokenizers.Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    id_seq_list = []\n",
    "\n",
    "    model_att = Model(inputs=model.input, \\\n",
    "                            outputs=[model.output, model.get_layer('att').output[-1]])\n",
    "    \n",
    "    for i in range(len(seq)):\n",
    "        start_counter = subseq_len\n",
    "        tokenized_sample = np.trim_zeros(seq[i])\n",
    "\n",
    "        label_probs, attentions = model_att.predict([seq[i:i+1], cons[i:i+1], sec[i:i+1]])\n",
    "\n",
    "        # Get decoded text and labels\n",
    "        id2word = dict(map(reversed, tokenizer.get_vocab().items()))\n",
    "        decoded_text = [id2word[word] for word in tokenized_sample] \n",
    "\n",
    "        # Get classification\n",
    "        label = (label_probs>0.5).astype(int).squeeze()\n",
    "\n",
    "        attentions_text = attentions[0,:len(tokenized_sample)]\n",
    "        attentions_text = (attentions_text - np.min(attentions_text)) / (np.max(attentions_text) - np.min(attentions_text))\n",
    "\n",
    "        tok_att_list = []\n",
    "        for token, attention_score in zip(decoded_text, attentions_text):\n",
    "            tok_att_list.append([token, attention_score])\n",
    "\n",
    "        unified_att_nts = []\n",
    "        for i in tok_att_list:\n",
    "            tok, score = i\n",
    "            nts = [nt for nt in tok]\n",
    "            for nt in nts:\n",
    "                unified_att_nts.append([nt, score])\n",
    "        \n",
    "        att_list = []\n",
    "        for i in range(len(unified_att_nts)):\n",
    "            _, attention = unified_att_nts[i]\n",
    "            att_list.append(attention)\n",
    "\n",
    "        maximum = 0\n",
    "        max_i = 0\n",
    "        for i in range(len(att_list) - start_counter + 1):\n",
    "            slide = sum(att_list[i : i + start_counter])\n",
    "            if slide > maximum:\n",
    "                maximum = slide\n",
    "                max_i = i\n",
    "\n",
    "        max_seq = ''\n",
    "        for tok, _ in unified_att_nts[max_i : max_i + start_counter]:\n",
    "            max_seq += tok\n",
    "            \n",
    "        while len(max_seq) > subseq_len:\n",
    "            \n",
    "            start_counter = start_counter - 1\n",
    "            max_seq = ''\n",
    "            \n",
    "            maximum = 0\n",
    "            max_i = 0\n",
    "            for i in range(len(att_list)-start_counter+ 1):\n",
    "                slide = sum(att_list[i:i+start_counter])\n",
    "                if slide > maximum:\n",
    "                    maximum = slide\n",
    "                    max_i = i  \n",
    "            for nt, _ in unified_att_nts[max_i:max_i+start_counter]:\n",
    "                max_seq += nt\n",
    "                \n",
    "        whole_seq = ''\n",
    "        for nt, _ in unified_att_nts:\n",
    "            whole_seq += nt\n",
    "\n",
    "        id_seq_list.append([int(label), max_seq, float(label_probs), whole_seq])\n",
    "    \n",
    "    \n",
    "    pos_id_seqs = []\n",
    "    \n",
    "    for i in id_seq_list:\n",
    "        if i[0] == 1:\n",
    "            pos_id_seqs.append([i[1], i[3], i[2]])\n",
    "    \n",
    "    pos_id_seqs = sort_based_on_pred_score(pos_id_seqs, True)\n",
    "    \n",
    "    if len(pos_id_seqs) >= number_of_seqs:\n",
    "        pos_id_seqs = pos_id_seqs[:number_of_seqs]\n",
    "\n",
    "    pos_pred_subseqs = [x[0] for x in pos_id_seqs]\n",
    "   \n",
    "    min_length = subseq_len\n",
    "    \n",
    "    return replace_ts_and_same_len(pos_pred_subseqs, min_length)\n",
    "\n",
    "def produce_seq_logo(biopython_pwm, bits=True, filename='None'):\n",
    "    pwm_dict = dict(biopython_pwm)\n",
    "    As = list(pwm_dict['A'])\n",
    "    Cs = list(pwm_dict['C'])\n",
    "    Gs = list(pwm_dict['G'])\n",
    "    Us = list(pwm_dict['U'])\n",
    "\n",
    "    pwm_arr = np.zeros((len(As), 4))\n",
    "    for i in range(len(As)):\n",
    "        pwm_arr[i][0] = round(As[i], 2)\n",
    "        pwm_arr[i][1] = round(Cs[i], 2)\n",
    "        pwm_arr[i][2] = round(Gs[i], 2)\n",
    "        pwm_arr[i][3] = round(Us[i], 2)\n",
    "\n",
    "    ppm = seqlogo.Ppm(pwm_arr, alphabet_type='RNA')\n",
    "    pwm = seqlogo.ppm2pwm(ppm)\n",
    "    pwm = seqlogo.Pwm(pwm, alphabet_type=\"RNA\")\n",
    "    print(pwm)\n",
    "    print(np.asarray(pwm))\n",
    "    seqlogo.seqlogo(pwm, ic_scale = bits, format = 'png', size = 'medium', filename=filename)\n",
    "\n",
    "def mostAbundantKmers(sequences, kmerLen):\n",
    "    kmerCounter = dict()\n",
    "    for seq in sequences:\n",
    "        if seq not in kmerCounter.keys():\n",
    "            kmerCounter[seq] = 1\n",
    "        else:\n",
    "            kmerCounter[seq] += 1\n",
    "    \n",
    "    sortedCounter = sorted(kmerCounter.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    return sortedCounter\n",
    "\n",
    "def seq_mers(seq, index, kmer_len=6):\n",
    "    subseqs = []\n",
    "    for i in range(len(seq) - kmer_len + 1):\n",
    "        subseqs.append(f'>{index}_{i}\\n{seq[i:i+kmer_len]}\\n')\n",
    "    return subseqs\n",
    "\n",
    "def getSequenceLogo(sequence, outputPath):\n",
    "    matrix = logomaker.sequence_to_matrix(sequence)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=[4,2])\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    logo = logomaker.Logo(matrix,\n",
    "                          ax=ax,\n",
    "                          color_scheme='classic',\n",
    "                          baseline_width=0,\n",
    "                          font_name='FreeSans',\n",
    "                          show_spines=False,\n",
    "                          width=.90)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    logo.fig.savefig(outputPath, bbox_inches='tight', dpi=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE FASTA FILES WITH TOP 50 PREDS PER CLASS\n",
    "### IMPORTANT SUBSEQUENCES (BASED ON ATTENTION SCORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_len = 6\n",
    "subseq_len = 20\n",
    "number_of_seqs = 50\n",
    "\n",
    "TOK_PATH = 'data/tokenizers/chooseYourTokenizer'\n",
    "FINETUNED_MODELS_PATH = Path(f'PATH/TO/MODELS/DIR')\n",
    "PATH_TO_EVAL_DATA = Path('PATH/TO/DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots_to_train = [os.path.basename(f) for f in os.scandir(FINETUNED_MODELS_PATH) if f.is_dir()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedCounter = []\n",
    "for prot in prots_to_train:\n",
    "    prot_dir_path = FINETUNED_MODELS_PATH / prot\n",
    "    for model_path in prot_dir_path.glob('*.h5'):\n",
    "        model_name = model_path.stem\n",
    "        \n",
    "        data = PATH_TO_EVAL_DATA / prot / 'dev.tsv'\n",
    "        \n",
    "        seqs, cons, secs, y = load_and_prepare(data, TOK_PATH, 150)\n",
    "        model = load_model(model_path, custom_objects={'Attention': Attention})\n",
    "        tensorflow.keras.backend.set_value(model.optimizer.lr, 0.001)\n",
    "        model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='Adamax')\n",
    "        imp_pos = get_important_subseqs_and_corresp_seqs(TOK_PATH,\n",
    "                                            seqs,\n",
    "                                            cons,\n",
    "                                            secs, \n",
    "                                            model, \n",
    "                                            subseq_len=subseq_len,\n",
    "                                            number_of_seqs=number_of_seqs)\n",
    "        outf_p = prot_dir_path / model_name\n",
    "        outf_p.mkdir(exist_ok=True, parents=True)\n",
    "              \n",
    "        impSubSeqsPath = outf_p / f'{prot}_{number_of_seqs}_{subseq_len}.fa'\n",
    "        with open(impSubSeqsPath, 'w') as outf:\n",
    "            for index, el in enumerate(imp_pos):\n",
    "                subseqs = seq_mers(el, index, kmer_len)\n",
    "                for i in subseqs:\n",
    "                    outf.write(i)\n",
    "                \n",
    "        seqs = []\n",
    "        for seq_record in SeqIO.parse(impSubSeqsPath, \"fasta\"):\n",
    "            seqs.append(str(seq_record.seq))\n",
    "        \n",
    "        sortedCounter = mostAbundantKmers(seqs, kmer_len)[:3] # most important 3 kmers\n",
    "\n",
    "        for i, s in enumerate(sortedCounter):\n",
    "            seq = s[0]\n",
    "            outp = outf_p / f'logo_{i}.png'\n",
    "            getSequenceLogo(seq, outp)\n",
    "\n",
    "        plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('py37-nn': conda)",
   "name": "python377jvsc74a57bd0ae6e16355e52c25c2bfe74bed653e12776f664ec4c2172129d8d42dec8dcedea"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}